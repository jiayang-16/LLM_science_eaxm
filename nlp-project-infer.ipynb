{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":3680037,"sourceType":"datasetVersion","datasetId":2202288},{"sourceId":4988409,"sourceType":"datasetVersion","datasetId":2893282},{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":6146317,"sourceType":"datasetVersion","datasetId":3524699},{"sourceId":6149251,"sourceType":"datasetVersion","datasetId":3526632},{"sourceId":6170048,"sourceType":"datasetVersion","datasetId":3540267},{"sourceId":6315195,"sourceType":"datasetVersion","datasetId":3626780},{"sourceId":6359953,"sourceType":"datasetVersion","datasetId":3663541},{"sourceId":6572938,"sourceType":"datasetVersion","datasetId":3600418},{"sourceId":7636679,"sourceType":"datasetVersion","datasetId":4450314},{"sourceId":163383055,"sourceType":"kernelVersion"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/d/rosekillerx/tokenizer-0-13-3/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-20T04:28:28.642846Z","iopub.execute_input":"2024-02-20T04:28:28.643845Z","iopub.status.idle":"2024-02-20T04:29:18.976160Z","shell.execute_reply.started":"2024-02-20T04:28:28.643803Z","shell.execute_reply":"2024-02-20T04:29:18.974956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\n\nimport faiss\nfrom faiss import write_index, read_index\n\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:29:18.978153Z","iopub.execute_input":"2024-02-20T04:29:18.978486Z","iopub.status.idle":"2024-02-20T04:29:18.985863Z","shell.execute_reply.started":"2024-02-20T04:29:18.978457Z","shell.execute_reply":"2024-02-20T04:29:18.984883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 3,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Main helper function to process documents from the EMR.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param document_type: String denoting the document type to be processed\n    :param document_sections: List of sections for a given document type to process\n    :param split_sentences: Flag to determine whether to further split sections into sentences\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df\n\n\ndef sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Obtains the sections of the imaging reports and returns only the \n    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param disable_progress_bar: Flag to disable tqdm progress bar\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df\n\n\ndef sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 3,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    Split a document into sentences. Can be used with `sectionize_documents`\n    to further split documents into more manageable pieces. Takes in offsets\n    to ensure that after splitting, the sentences can be matched to the\n    location in the original documents.\n\n    :param documents: Iterable containing documents which are strings\n    :param document_ids: Iterable containing document unique identifiers\n    :param offsets: Iterable tuple of the start and end indices\n    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:29:18.987334Z","iopub.execute_input":"2024-02-20T04:29:18.987617Z","iopub.status.idle":"2024-02-20T04:29:19.004641Z","shell.execute_reply.started":"2024-02-20T04:29:18.987594Z","shell.execute_reply":"2024-02-20T04:29:19.003929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)\nDEVICE = 0\nINFER = True\nSUBMISSION = False","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:29:19.006674Z","iopub.execute_input":"2024-02-20T04:29:19.006967Z","iopub.status.idle":"2024-02-20T04:29:19.017788Z","shell.execute_reply.started":"2024-02-20T04:29:19.006944Z","shell.execute_reply":"2024-02-20T04:29:19.017023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:29:19.019364Z","iopub.execute_input":"2024-02-20T04:29:19.019626Z","iopub.status.idle":"2024-02-20T04:29:19.026202Z","shell.execute_reply.started":"2024-02-20T04:29:19.019604Z","shell.execute_reply":"2024-02-20T04:29:19.025380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# initial search with title and the first sentence","metadata":{}},{"cell_type":"code","source":"# trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\").drop(\"id\", axis=1)\nif SUBMISSION:\n    trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", axis=1)\nelse:\n    trn = pd.read_csv(\"/kaggle/input/15k-high-quality-examples/15k_gpt3.5-turbo.csv\").iloc[:2000].dropna().reset_index(drop=True)\n#     trn = pd.concat([trn.iloc[:400],pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\").drop(\"id\", axis=1)])\n    \ntrn.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:29:19.027190Z","iopub.execute_input":"2024-02-20T04:29:19.027468Z","iopub.status.idle":"2024-02-20T04:29:19.197829Z","shell.execute_reply.started":"2024-02-20T04:29:19.027445Z","shell.execute_reply":"2024-02-20T04:29:19.196855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if INFER:\n    title_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n    model = SentenceTransformer(SIM_MODEL, device='cuda')\n    model.max_seq_length = 384\n    model = model.half()\n    prompt_embeddings = model.encode(trn.prompt.values, batch_size=32, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n    prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n    gc.collect()\n\n    ss, si = title_index.search(prompt_embeddings, 5)\n    del title_index\n    del prompt_embeddings\n    gc.collect()\n    libc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:29:19.199232Z","iopub.execute_input":"2024-02-20T04:29:19.199854Z","iopub.status.idle":"2024-02-20T04:30:23.633486Z","shell.execute_reply.started":"2024-02-20T04:29:19.199818Z","shell.execute_reply":"2024-02-20T04:30:23.632660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import details of articles selected in initial search","metadata":{}},{"cell_type":"code","source":"if INFER:\n    index_df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                         columns=['id', 'file'])\n\n    file_data = []\n    for i,index in enumerate(si):\n        temp = index_df.loc[index].copy()\n        temp[\"prompt_id\"] = i\n        file_data.append(temp)\n    print(file_data[0])\n    file_data = pd.concat(file_data).reset_index(drop=True)\n    file_data = file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n    del index_df\n    gc.collect()\n\n    text_data = []\n    for file in tqdm(file_data.file.unique(),total=len(file_data.file.unique())):\n        text_df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n        query_res = pd.merge(file_data[file_data['file']==file], text_df, on='id',how='inner')\n        del text_df\n        gc.collect()\n        text_data.append(query_res)\n    text_data = pd.concat(text_data).drop([\"prompt_id\",\"file\"],axis=1).drop_duplicates().reset_index(drop=True)\n    gc.collect()\n\n    libc.malloc_trim(0)\n\n    processed_text_data = process_documents(text_data.text.values,text_data.id.values)\n    print(processed_text_data.head(10))\n\n    text_data_embeddings = model.encode(processed_text_data.text,\n                                        batch_size=32,\n                                        device=DEVICE,\n                                        show_progress_bar=True,\n                                        convert_to_tensor=True,\n                                        normalize_embeddings=True)\n    text_data_embeddings = text_data_embeddings.detach().cpu().numpy()\n\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:30:23.634629Z","iopub.execute_input":"2024-02-20T04:30:23.634900Z","iopub.status.idle":"2024-02-20T04:35:55.034791Z","shell.execute_reply.started":"2024-02-20T04:30:23.634865Z","shell.execute_reply":"2024-02-20T04:35:55.033944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Advanced Search","metadata":{}},{"cell_type":"code","source":"if INFER:\n    trn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n    trn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']\n    question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=32, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n    question_embeddings = question_embeddings.detach().cpu().numpy()\n\n    NUM_SENTENCES_INCLUDE = 20\n    contexts = []\n\n    for r in tqdm(trn.itertuples(), total=len(trn)):\n\n        prompt_id = r.Index\n\n        prompt_indices = processed_text_data[processed_text_data['document_id'].isin(file_data[file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n        if prompt_indices.shape[0] > 0:\n            prompt_index = faiss.index_factory(text_data_embeddings.shape[1], \"Flat\")\n            prompt_index.add(text_data_embeddings[prompt_indices])\n\n            context = \"\"\n\n            ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n            for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n                context += processed_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n\n        contexts.append(context)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:35:55.036005Z","iopub.execute_input":"2024-02-20T04:35:55.036280Z","iopub.status.idle":"2024-02-20T04:36:08.584977Z","shell.execute_reply.started":"2024-02-20T04:35:55.036257Z","shell.execute_reply":"2024-02-20T04:36:08.584109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_trn = trn.copy()\nif INFER:\n    trn[\"context\"] = contexts\n    new_trn[\"prompt\"] = trn[\"context\"].apply(lambda x: x[:2000]) +\" #### \"+trn[\"prompt\"]\nif SUBMISSION:\n    new_trn[\"answer\"] = \"A\"\nelse:\n    new_trn.to_csv(\"train_with_infer_15k.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:36:08.589262Z","iopub.execute_input":"2024-02-20T04:36:08.589849Z","iopub.status.idle":"2024-02-20T04:36:08.697111Z","shell.execute_reply.started":"2024-02-20T04:36:08.589820Z","shell.execute_reply":"2024-02-20T04:36:08.696145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_trn.info()\nbreak","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:36:08.698256Z","iopub.execute_input":"2024-02-20T04:36:08.698540Z","iopub.status.idle":"2024-02-20T04:36:08.716376Z","shell.execute_reply.started":"2024-02-20T04:36:08.698503Z","shell.execute_reply":"2024-02-20T04:36:08.715475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from peft import PeftModel,PeftConfig\n# peft_model_dir = \"/kaggle/input/nlp-project-train/model_v1\"\n# base_model_dir = \"/kaggle/input/deberta-v3-large-hf-weights\"\n# config = PeftConfig.from_pretrained(peft_model_dir)\n# tokenizer = AutoTokenizer.from_pretrained(base_model_dir)\n# model = AutoModelForMultipleChoice.from_pretrained(base_model_dir).cuda()\n# model = PeftModel.from_pretrained(model, peft_model_dir)\n# model.eval()\nbase_model_dir = \"/kaggle/input/deberta-v3-large-hf-weights\"\nmodel_dir = \"/kaggle/input/llm-science-run-context-2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:36:08.717742Z","iopub.execute_input":"2024-02-20T04:36:08.718158Z","iopub.status.idle":"2024-02-20T04:36:36.485012Z","shell.execute_reply.started":"2024-02-20T04:36:08.718126Z","shell.execute_reply":"2024-02-20T04:36:36.484073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\n\noption_to_index = {option: idx for idx, option in enumerate('ABCDE')}\nindex_to_option = {v: k for k,v in option_to_index.items()}\n\ndef preprocess(example):\n    first_sentence = [example['prompt']] * 5\n    second_sentences = [example[option] for option in 'ABCDE']\n    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    \n    return tokenized_example\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:36:36.486057Z","iopub.execute_input":"2024-02-20T04:36:36.486327Z","iopub.status.idle":"2024-02-20T04:36:36.498195Z","shell.execute_reply.started":"2024-02-20T04:36:36.486305Z","shell.execute_reply":"2024-02-20T04:36:36.497235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_trn = pd.read_csv(\"/kaggle/working/train_with_infer_15k.csv\")\nnew_trn.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:36:36.499383Z","iopub.execute_input":"2024-02-20T04:36:36.500436Z","iopub.status.idle":"2024-02-20T04:36:36.549658Z","shell.execute_reply.started":"2024-02-20T04:36:36.500403Z","shell.execute_reply":"2024-02-20T04:36:36.548931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = Dataset.from_pandas(new_trn[[\"prompt\",\"A\",\"B\",\"C\",\"D\",\"E\",\"answer\"]])\ntokenized_test = test.map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E',\"answer\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test, batch_size=1, shuffle=False, collate_fn=data_collator)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:36:36.550843Z","iopub.execute_input":"2024-02-20T04:36:36.551392Z","iopub.status.idle":"2024-02-20T04:36:40.560849Z","shell.execute_reply.started":"2024-02-20T04:36:36.551365Z","shell.execute_reply":"2024-02-20T04:36:40.559915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = []\nfor batch in tqdm(test_dataloader):\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions.append(outputs.logits.cpu().detach())\n\ntest_predictions = torch.cat(test_predictions)\ntest_predictions = test_predictions.numpy()","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:36:40.562088Z","iopub.execute_input":"2024-02-20T04:36:40.562383Z","iopub.status.idle":"2024-02-20T04:40:44.207715Z","shell.execute_reply.started":"2024-02-20T04:36:40.562357Z","shell.execute_reply":"2024-02-20T04:40:44.206922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_letter = np.array(list('ABCDE'))[np.argsort(-test_predictions, 1)]","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:40:44.209342Z","iopub.execute_input":"2024-02-20T04:40:44.209635Z","iopub.status.idle":"2024-02-20T04:40:44.214209Z","shell.execute_reply.started":"2024-02-20T04:40:44.209610Z","shell.execute_reply":"2024-02-20T04:40:44.213360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def precision_at_k(r, k):\n    \"\"\"Precision at k\"\"\"\n    assert k <= len(r)\n    assert k != 0\n    return sum(int(x) for x in r[:k]) / k\n\ndef MAP_at_3(predictions, true_items):\n    \"\"\"Score is mean average precision at 3\"\"\"\n    U = len(predictions)\n    map_at_3 = 0.0\n    for u in range(U):\n        user_preds = predictions[u]\n        user_true = true_items[u]\n        user_results = [1 if item == user_true else 0 for item in user_preds]\n        for k in range(min(len(user_preds), 3)):\n            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n    return map_at_3 / U","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:40:44.215398Z","iopub.execute_input":"2024-02-20T04:40:44.216167Z","iopub.status.idle":"2024-02-20T04:40:44.229037Z","shell.execute_reply.started":"2024-02-20T04:40:44.216142Z","shell.execute_reply":"2024-02-20T04:40:44.228236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAP_at_3(prediction_letter, new_trn[\"answer\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:40:44.230100Z","iopub.execute_input":"2024-02-20T04:40:44.230361Z","iopub.status.idle":"2024-02-20T04:40:44.250947Z","shell.execute_reply.started":"2024-02-20T04:40:44.230339Z","shell.execute_reply":"2024-02-20T04:40:44.250096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_as_string = trn['prediction'] = [\n    ' '.join(row) for row in prediction_letter[:, :3]\n]","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:40:44.252153Z","iopub.execute_input":"2024-02-20T04:40:44.252745Z","iopub.status.idle":"2024-02-20T04:40:44.263273Z","shell.execute_reply.started":"2024-02-20T04:40:44.252721Z","shell.execute_reply":"2024-02-20T04:40:44.262411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = trn[['prediction']].reset_index()\nsubmission.rename(columns={'index':'id'}, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T04:40:44.264179Z","iopub.execute_input":"2024-02-20T04:40:44.264434Z","iopub.status.idle":"2024-02-20T04:40:44.277105Z","shell.execute_reply.started":"2024-02-20T04:40:44.264412Z","shell.execute_reply":"2024-02-20T04:40:44.276224Z"},"trusted":true},"execution_count":null,"outputs":[]}]}